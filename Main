import numpy as np
from pylab import plot, show, grid, xlabel, ylabel
from scipy import stats
import scipy
import MathematicalModel
import NN
import Config

import torch
import torch.nn as nn
import torch.nn.functional as F
import time
import logging as log
import coloredlogs


class MainRoutine:
    def __init__(self, config):
        print("Main Routine init wird aufgerufen")
        np.random.seed(seed=config.random_seed)

        self.Model = MathematicalModel.MathematicalModel(config.T, config.d, config.mu, config.sigma, config.g, config.xi)
        self.NN = NN.NN(c1, self.Model)
        self.config = config

        self.N = config.N

    def MainSchleife(self):
        M = 30  # Number of optimization steps
        J = 16  # Batchsize
        L = 32  # valsize

        # self.NN.test()

        path_list, individual_payoffs, average_payoff, val_value_list = self.NN.optimization(M, J, L)
        for k in range(len(val_value_list)):
            log.info("Value of Option is approximately %s after %s steps" % (val_value_list[k].item(), k))

        # TODO: Here i should make a new monte carlo approximation of the payoff after the net stopped optimizing.
        # TODO: Nevermind. testset vs trainset, i should structre it differently alltogether. train vs test vs validation
        # log.debug("max is %s\n", torch.max(torch.stack(val_value_list)).item())
        if self.config.other_computation_exists:
            log.info("other computation yields: %s", self.config.other_computation)
        """
        for l in range(path_list.__len__() - 1):
            h = path_list[l + 1]
            self.draw(path_list[l + 1])
        """

    def myprint(self, *argv):
        out = ''.join(str(s) for s in argv)
        out += "\n"
        print(out)

    def draw(self, x):
        # Diese Funktion stoppt den Programmfluss bis ich die Graphik geschlossen habe

        t = np.linspace(0.0, self.Model.getT(), self.N + 1)
        for k in range(self.Model.getd()):
            plot(t, x[k])
        xlabel('t', fontsize=16)
        ylabel('x', fontsize=16)
        grid(True)
        show()


class Tutorial:
    def __init__(self):

        x = torch.rand(5, 3)

        # let us run this cell only if CUDA is available
        # We will use ``torch.device`` objects to move tensors in and out of GPU
        if torch.cuda.is_available():
            device = torch.device("cuda")  # a CUDA device object
            y = torch.ones_like(x, device=device)  # directly create a tensor on GPU
            x = x.to(device)  # or just use strings ``.to("cuda")``
            z = x + y
            print(z)
            print(z.to("cpu", torch.double))


        class Net(nn.Module):

            def __init__(self):
                super(Net, self).__init__()
                # 1 input image channel, 6 output channels, 3x3 square convolution
                # kernel
                self.conv1 = nn.Conv2d(1, 6, 3)
                self.conv2 = nn.Conv2d(6, 16, 3)
                # an affine operation: y = Wx + b
                self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension
                self.fc2 = nn.Linear(120, 84)
                self.fc3 = nn.Linear(84, 10)

            def forward(self, x):
                # Max pooling over a (2, 2) window
                x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
                # If the size is a square you can only specify a single number
                x = F.max_pool2d(F.relu(self.conv2(x)), 2)
                x = x.view(-1, self.num_flat_features(x))
                x = F.relu(self.fc1(x))
                x = F.relu(self.fc2(x))
                x = self.fc3(x)
                return x

            def num_flat_features(self, x):
                size = x.size()[1:]  # all dimensions except the batch dimension
                num_features = 1
                for s in size:
                    num_features *= s
                return num_features


        net = Net()
        print(net)

        params = list(net.parameters())
        print(len(params))
        print(params[0].size())  # conv1's .weight

        input = torch.randn(1, 1, 32, 32)
        out = net(input)
        print(out)

        input = torch.randn(1, 1, 32, 32)
        out = net(input)
        print(out)


if __name__ == '__main__':
    # filename='example.log'
    # stream=sys.stderr,
    # log.basicConfig(level=log.DEBUG, format='%(asctime)s %(message)s')
    coloredlogs.install(level='DEBUG', fmt='%(asctime)s %(message)s')

    start_time = time.time()
    config_time = time.time()

    c1 = Config.Config("1")
    log.info("time for config is %s seconds" % (time.time() - config_time))
    """
    log.debug("hi")
    log.critical("iovbu")
    log.warning("onv√∂dsgo")
    log.error("zkigbaezg")
    """
    # NN = NN.NN(c1)
    # NN.test2()

    # lokaleMainRoutine = MainRoutine(c1)
    # lokaleMainRoutine.MainSchleife()

    log.info("time for everything is %s seconds" % (time.time() - start_time))
